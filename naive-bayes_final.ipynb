{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-09T06:35:57.019070Z","iopub.execute_input":"2024-10-09T06:35:57.019383Z","iopub.status.idle":"2024-10-09T06:35:58.009641Z","shell.execute_reply.started":"2024-10-09T06:35:57.019351Z","shell.execute_reply":"2024-10-09T06:35:58.008610Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/deepakachu5114/ML-Project.git\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:35:58.010949Z","iopub.execute_input":"2024-10-09T06:35:58.011344Z","iopub.status.idle":"2024-10-09T06:35:59.928700Z","shell.execute_reply.started":"2024-10-09T06:35:58.011305Z","shell.execute_reply":"2024-10-09T06:35:59.927530Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'ML-Project'...\nremote: Enumerating objects: 35, done.\u001b[K\nremote: Counting objects: 100% (35/35), done.\u001b[K\nremote: Compressing objects: 100% (27/27), done.\u001b[K\nremote: Total 35 (delta 7), reused 21 (delta 3), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (35/35), 1.33 MiB | 17.40 MiB/s, done.\nResolving deltas: 100% (7/7), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd \"/kaggle/working/ML-Project\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:35:59.931657Z","iopub.execute_input":"2024-10-09T06:35:59.932036Z","iopub.status.idle":"2024-10-09T06:35:59.940271Z","shell.execute_reply.started":"2024-10-09T06:35:59.932000Z","shell.execute_reply":"2024-10-09T06:35:59.939075Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:35:59.941614Z","iopub.execute_input":"2024-10-09T06:35:59.941992Z","iopub.status.idle":"2024-10-09T06:36:13.283068Z","shell.execute_reply.started":"2024-10-09T06:35:59.941944Z","shell.execute_reply":"2024-10-09T06:36:13.281949Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.3->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile naive_bayes.py\nfrom preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\nfrom sentence_transformers import SentenceTransformer\n\npreproc_noncontextual = PreprocessForNonContextualEmbeddings()\npreproc_noncontextual.save()\n\npreproc_contextual = PreprocessForContextualEmbeddings()\npreproc_contextual.save()\n\n# generating embeddings\n\nfrom embeddings import TextEmbeddings\n\n# preprocessed data\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# tfidf embeddings\ntfidf_train, tfidf_test = embeddings.apply_tfidf()\n\n# word2vec embeddings\nword2vec_train, word2vec_test = embeddings.apply_word2vec()\n\n# stransformers embeddings\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\n\nsentence_transformer_train, sentence_transformer_test = embeddings_contextual.apply_sentence_transformer()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:36:13.284817Z","iopub.execute_input":"2024-10-09T06:36:13.285235Z","iopub.status.idle":"2024-10-09T06:36:13.293452Z","shell.execute_reply.started":"2024-10-09T06:36:13.285186Z","shell.execute_reply":"2024-10-09T06:36:13.292524Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing naive_bayes.py\n","output_type":"stream"}]},{"cell_type":"code","source":"cd \"/kaggle/working/ML-Project/data\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:36:13.294845Z","iopub.execute_input":"2024-10-09T06:36:13.295132Z","iopub.status.idle":"2024-10-09T06:36:13.306055Z","shell.execute_reply.started":"2024-10-09T06:36:13.295102Z","shell.execute_reply":"2024-10-09T06:36:13.305157Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project/data\n","output_type":"stream"}]},{"cell_type":"code","source":"mkdir preprocessed\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:36:13.307323Z","iopub.execute_input":"2024-10-09T06:36:13.307685Z","iopub.status.idle":"2024-10-09T06:36:14.302487Z","shell.execute_reply.started":"2024-10-09T06:36:13.307644Z","shell.execute_reply":"2024-10-09T06:36:14.301218Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"cd \"/kaggle/working/ML-Project\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:36:14.306136Z","iopub.execute_input":"2024-10-09T06:36:14.306482Z","iopub.status.idle":"2024-10-09T06:36:14.312764Z","shell.execute_reply.started":"2024-10-09T06:36:14.306445Z","shell.execute_reply":"2024-10-09T06:36:14.311744Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project\n","output_type":"stream"}]},{"cell_type":"code","source":"!python naive_bayes.py\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:36:14.313800Z","iopub.execute_input":"2024-10-09T06:36:14.314067Z","iopub.status.idle":"2024-10-09T06:37:57.331603Z","shell.execute_reply.started":"2024-10-09T06:36:14.314037Z","shell.execute_reply":"2024-10-09T06:37:57.330302Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\u001b[38;20m2024-10-09 06:36:38,884 - logger_config - INFO - Preprocessing for non-contextual embeddings initialized. (preprocess.py:20)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:21,707 - logger_config - INFO - Preprocessing for non-contextual embeddings completed. (preprocess.py:85)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:21,800 - logger_config - INFO - Preprocessing for contextual embeddings initialized. (preprocess.py:92)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:21,936 - logger_config - INFO - Preprocessing for contextual embeddings completed. (preprocess.py:138)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:45,812 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:46,288 - logger_config - INFO - TF-IDF embeddings generated successfully. (embeddings.py:23)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:47,712 - logger_config - INFO - Word2Vec embeddings generated successfully. (embeddings.py:44)\u001b[0m\n\u001b[38;20m2024-10-09 06:37:47,742 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 2.48MB/s]\nconfig_sentence_transformers.json: 100%|████████| 116/116 [00:00<00:00, 729kB/s]\nREADME.md: 100%|███████████████████████████| 10.7k/10.7k [00:00<00:00, 54.1MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 347kB/s]\nconfig.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 3.84MB/s]\nmodel.safetensors: 100%|████████████████████| 90.9M/90.9M [00:00<00:00, 339MB/s]\ntokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 2.54MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 22.6MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 64.4MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 910kB/s]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n1_Pooling/config.json: 100%|███████████████████| 190/190 [00:00<00:00, 1.38MB/s]\nBatches: 100%|██████████████████████████████████| 89/89 [00:02<00:00, 40.92it/s]\nBatches: 100%|██████████████████████████████████| 23/23 [00:00<00:00, 53.42it/s]\n\u001b[38;20m2024-10-09 06:37:54,810 - logger_config - INFO - SentenceTransformer embeddings generated successfully. (embeddings.py:59)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from embeddings import TextEmbeddings\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:37:57.333455Z","iopub.execute_input":"2024-10-09T06:37:57.334413Z","iopub.status.idle":"2024-10-09T06:38:13.200233Z","shell.execute_reply.started":"2024-10-09T06:37:57.334360Z","shell.execute_reply":"2024-10-09T06:38:13.199467Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# tfidf embeddings\ntfidf_train, tfidf_test = embeddings.apply_tfidf()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:38:13.201378Z","iopub.execute_input":"2024-10-09T06:38:13.201999Z","iopub.status.idle":"2024-10-09T06:38:16.411165Z","shell.execute_reply.started":"2024-10-09T06:38:13.201963Z","shell.execute_reply":"2024-10-09T06:38:16.410231Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:38:15,921 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:38:16,407 - logger_config - INFO - TF-IDF embeddings generated successfully. (embeddings.py:23)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# word2vec embeddings\nword2vec_train, word2vec_test = embeddings.apply_word2vec()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:38:16.412366Z","iopub.execute_input":"2024-10-09T06:38:16.412695Z","iopub.status.idle":"2024-10-09T06:38:17.829587Z","shell.execute_reply.started":"2024-10-09T06:38:16.412660Z","shell.execute_reply":"2024-10-09T06:38:17.828852Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:38:16,434 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:38:17,821 - logger_config - INFO - Word2Vec embeddings generated successfully. (embeddings.py:44)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\n\nsentence_transformer_train, sentence_transformer_test = embeddings_contextual.apply_sentence_transformer()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:38:17.830651Z","iopub.execute_input":"2024-10-09T06:38:17.830988Z","iopub.status.idle":"2024-10-09T06:38:21.933787Z","shell.execute_reply.started":"2024-10-09T06:38:17.830954Z","shell.execute_reply":"2024-10-09T06:38:21.932972Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:38:17,855 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:38:17,882 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/89 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37e4b3761db04405898e52bac77a29a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aec7a0df6aa043d2849ea8f51aed3cf4"}},"metadata":{}},{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:38:21,925 - logger_config - INFO - SentenceTransformer embeddings generated successfully. (embeddings.py:59)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\nfrom sentence_transformers import SentenceTransformer\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\nmodel_mpnet = SentenceTransformer('all-mpnet-base-v2')\ntrain_embeddings_mpnet = model_mpnet.encode(train_data_contextual['text'].tolist(), show_progress_bar=True)\ntest_embeddings_mpnet = model_mpnet.encode(test_data_contextual['text'].tolist(), show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:38:21.934990Z","iopub.execute_input":"2024-10-09T06:38:21.935310Z","iopub.status.idle":"2024-10-09T06:38:42.710289Z","shell.execute_reply.started":"2024-10-09T06:38:21.935276Z","shell.execute_reply":"2024-10-09T06:38:42.709162Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:38:21,961 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:38:21,986 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2e214dca6a465eab9df6292c3f4ddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9be5d67eade4353b3f4dbb40fe536c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5142543f644aa0978b6c4c1d5478fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3539c4f29c8947e6bb2ae235cc04dec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"740a0bb55d0f4ed89485400fce4fbd10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583e950b983a4c7294e212bf735f2c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bad0e8756b5425b9fecfa9555b012d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380ef7adb70245408ac8fcee27f7444b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b36e504ddfe340b3ac8f1e75bc2fa51f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9d698e526c4c45a841437d65d1c2c4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2b8ad38270f442aa4e5d5559264a854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/89 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d058ce2b3e6446f90c57c3c9ddc4be8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce21325ebb14c25a5fbd458e4052e87"}},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB  # Import Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ndef train_and_evaluate_naive_bayes(X_train, y_train, X_test, y_test, embedding_name):\n    \"\"\"\n    Train and evaluate Naive Bayes model on the given data.\n    Arguments:\n    - X_train: Training features\n    - y_train: Training labels\n    - X_test: Test features\n    - y_test: Test labels\n    - embedding_name: Name of the embedding used\n    \n    Returns:\n    - results: Dictionary containing model and its performance metrics\n    \"\"\"\n\n    # Split the training data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n    # Initialize Naive Bayes model\n    model = GaussianNB()  # Change to Naive Bayes\n    \n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on training, validation, and test sets\n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    y_test_pred = model.predict(X_test)\n\n    # Function to calculate evaluation metrics\n    def get_metrics(y_true, y_pred):\n        return {\n            'Precision': precision_score(y_true, y_pred, average='weighted'),\n            'Recall': recall_score(y_true, y_pred, average='weighted'),\n            'F1 Score': f1_score(y_true, y_pred, average='weighted'),\n            'Accuracy': accuracy_score(y_true, y_pred)\n        }\n\n    # Compute metrics for training, validation, and test sets\n    train_metrics = get_metrics(y_train, y_train_pred)\n    val_metrics = get_metrics(y_val, y_val_pred)\n    test_metrics = get_metrics(y_test, y_test_pred)\n\n    # Print results for each set of embeddings\n    print(f\"\\nResults for {embedding_name} embeddings - Naive Bayes:\")\n\n    print(\"\\nTraining Metrics:\")\n    for metric, value in train_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    print(\"\\nValidation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    print(\"\\nTest Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    return model, test_metrics","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:38:42.712167Z","iopub.execute_input":"2024-10-09T06:38:42.712905Z","iopub.status.idle":"2024-10-09T06:38:42.728735Z","shell.execute_reply.started":"2024-10-09T06:38:42.712852Z","shell.execute_reply":"2024-10-09T06:38:42.727858Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_results = {}\n\nprint(\"Running Naive Bayes on TF-IDF Embeddings...\")\nall_results['TF-IDF'] = train_and_evaluate_naive_bayes(\n    tfidf_train, train_data['label'], tfidf_test, test_data['label'], \"TF-IDF\"\n)\n\nprint(\"\\nRunning Naive Bayes on Word2Vec Embeddings...\")\nall_results['Word2Vec'] = train_and_evaluate_naive_bayes(\n    word2vec_train, train_data['label'], word2vec_test, test_data['label'], \"Word2Vec\"\n)\n    \nprint(\"\\nRunning Naive Bayes on SentenceTransformer Embeddings...\")\nall_results['SentenceTransformer'] = train_and_evaluate_naive_bayes(\n    sentence_transformer_train, train_data_contextual['label'],\n    sentence_transformer_test, test_data_contextual['label'], \"SentenceTransformer\"\n)\n\nprint(\"\\nRunning Naive Bayes on all-mpnet-base-v2 Embeddings...\")\nall_results['all-mpnet-base-v2'] = train_and_evaluate_naive_bayes(\n    train_embeddings_mpnet, train_data_contextual['label'],\n    test_embeddings_mpnet, test_data_contextual['label'], \"all-mpnet-base-v2\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:40:51.137867Z","iopub.execute_input":"2024-10-09T06:40:51.138338Z","iopub.status.idle":"2024-10-09T06:40:53.035327Z","shell.execute_reply.started":"2024-10-09T06:40:51.138289Z","shell.execute_reply":"2024-10-09T06:40:53.034362Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Running Naive Bayes on TF-IDF Embeddings...\n\nResults for TF-IDF embeddings - Naive Bayes:\n\nTraining Metrics:\nPrecision: 0.9586\nRecall: 0.9551\nF1 Score: 0.9549\nAccuracy: 0.9551\n\nValidation Metrics:\nPrecision: 0.6288\nRecall: 0.6303\nF1 Score: 0.6273\nAccuracy: 0.6303\n\nTest Metrics:\nPrecision: 0.5501\nRecall: 0.5510\nF1 Score: 0.5459\nAccuracy: 0.5510\n\nRunning Naive Bayes on Word2Vec Embeddings...\n\nResults for Word2Vec embeddings - Naive Bayes:\n\nTraining Metrics:\nPrecision: 0.6135\nRecall: 0.6132\nF1 Score: 0.6094\nAccuracy: 0.6132\n\nValidation Metrics:\nPrecision: 0.5964\nRecall: 0.5986\nF1 Score: 0.5956\nAccuracy: 0.5986\n\nTest Metrics:\nPrecision: 0.5597\nRecall: 0.5566\nF1 Score: 0.5378\nAccuracy: 0.5566\n\nRunning Naive Bayes on SentenceTransformer Embeddings...\n\nResults for SentenceTransformer embeddings - Naive Bayes:\n\nTraining Metrics:\nPrecision: 0.7665\nRecall: 0.7656\nF1 Score: 0.7657\nAccuracy: 0.7656\n\nValidation Metrics:\nPrecision: 0.7041\nRecall: 0.7042\nF1 Score: 0.7041\nAccuracy: 0.7042\n\nTest Metrics:\nPrecision: 0.7260\nRecall: 0.7259\nF1 Score: 0.7259\nAccuracy: 0.7259\n\nRunning Naive Bayes on all-mpnet-base-v2 Embeddings...\n\nResults for all-mpnet-base-v2 embeddings - Naive Bayes:\n\nTraining Metrics:\nPrecision: 0.7719\nRecall: 0.7714\nF1 Score: 0.7715\nAccuracy: 0.7714\n\nValidation Metrics:\nPrecision: 0.7069\nRecall: 0.7060\nF1 Score: 0.7063\nAccuracy: 0.7060\n\nTest Metrics:\nPrecision: 0.7426\nRecall: 0.7413\nF1 Score: 0.7413\nAccuracy: 0.7413\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}