{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/deepakachu5114/ML-Project.git\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:21:12.718838Z","iopub.execute_input":"2024-10-08T18:21:12.719522Z","iopub.status.idle":"2024-10-08T18:21:14.768319Z","shell.execute_reply.started":"2024-10-08T18:21:12.719472Z","shell.execute_reply":"2024-10-08T18:21:14.767126Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'ML-Project'...\nremote: Enumerating objects: 31, done.\u001b[K\nremote: Counting objects: 100% (31/31), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 31 (delta 5), reused 22 (delta 3), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (31/31), 1.32 MiB | 3.33 MiB/s, done.\nResolving deltas: 100% (5/5), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd \"/kaggle/working/ML-Project\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:21:14.770456Z","iopub.execute_input":"2024-10-08T18:21:14.770888Z","iopub.status.idle":"2024-10-08T18:21:14.778811Z","shell.execute_reply.started":"2024-10-08T18:21:14.770839Z","shell.execute_reply":"2024-10-08T18:21:14.777605Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile naive_bayes.py\nfrom preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\n\n\npreproc_noncontextual = PreprocessForNonContextualEmbeddings()\npreproc_noncontextual.save()\n\npreproc_contextual = PreprocessForContextualEmbeddings()\npreproc_contextual.save()\n\n# generating embeddings\n\nfrom embeddings import TextEmbeddings\n\n# preprocessed data\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# tfidf embeddings\ntfidf_train, tfidf_test = embeddings.apply_tfidf()\n\n# word2vec embeddings\nword2vec_train, word2vec_test = embeddings.apply_word2vec()\n\n# stransformers embeddings\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\n\nsentence_transformer_train, sentence_transformer_test = embeddings_contextual.apply_sentence_transformer()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:21:38.324812Z","iopub.execute_input":"2024-10-08T18:21:38.325527Z","iopub.status.idle":"2024-10-08T18:21:38.332160Z","shell.execute_reply.started":"2024-10-08T18:21:38.325481Z","shell.execute_reply":"2024-10-08T18:21:38.331220Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing naive_bayes.py\n","output_type":"stream"}]},{"cell_type":"code","source":"cd \"/kaggle/working/ML-Project/data\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:21:46.964920Z","iopub.execute_input":"2024-10-08T18:21:46.965808Z","iopub.status.idle":"2024-10-08T18:21:46.971008Z","shell.execute_reply.started":"2024-10-08T18:21:46.965765Z","shell.execute_reply":"2024-10-08T18:21:46.970122Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project/data\n","output_type":"stream"}]},{"cell_type":"code","source":"mkdir preprocessed\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:21:58.407937Z","iopub.execute_input":"2024-10-08T18:21:58.408321Z","iopub.status.idle":"2024-10-08T18:21:59.385770Z","shell.execute_reply.started":"2024-10-08T18:21:58.408283Z","shell.execute_reply":"2024-10-08T18:21:59.384709Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cd \"/kaggle/working/ML-Project\"\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:22:11.380496Z","iopub.execute_input":"2024-10-08T18:22:11.381389Z","iopub.status.idle":"2024-10-08T18:22:11.386998Z","shell.execute_reply.started":"2024-10-08T18:22:11.381345Z","shell.execute_reply":"2024-10-08T18:22:11.386099Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install sentence-transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:22:19.824322Z","iopub.execute_input":"2024-10-08T18:22:19.825192Z","iopub.status.idle":"2024-10-08T18:22:32.435160Z","shell.execute_reply.started":"2024-10-08T18:22:19.825151Z","shell.execute_reply":"2024-10-08T18:22:32.434071Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.3->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!python naive_bayes.py\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:22:48.560425Z","iopub.execute_input":"2024-10-08T18:22:48.560859Z","iopub.status.idle":"2024-10-08T18:24:15.302340Z","shell.execute_reply.started":"2024-10-08T18:22:48.560819Z","shell.execute_reply":"2024-10-08T18:24:15.301119Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\u001b[38;20m2024-10-08 18:22:58,235 - logger_config - INFO - Preprocessing for non-contextual embeddings initialized. (preprocess.py:20)\u001b[0m\n\u001b[38;20m2024-10-08 18:23:37,624 - logger_config - INFO - Preprocessing for non-contextual embeddings completed. (preprocess.py:85)\u001b[0m\n\u001b[38;20m2024-10-08 18:23:37,712 - logger_config - INFO - Preprocessing for contextual embeddings initialized. (preprocess.py:92)\u001b[0m\n\u001b[38;20m2024-10-08 18:23:37,837 - logger_config - INFO - Preprocessing for contextual embeddings completed. (preprocess.py:138)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:04,888 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:05,355 - logger_config - INFO - TF-IDF embeddings generated successfully. (embeddings.py:23)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:06,677 - logger_config - INFO - Word2Vec embeddings generated successfully. (embeddings.py:44)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:06,705 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 2.50MB/s]\nconfig_sentence_transformers.json: 100%|████████| 116/116 [00:00<00:00, 932kB/s]\nREADME.md: 100%|███████████████████████████| 10.7k/10.7k [00:00<00:00, 61.8MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 452kB/s]\nconfig.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 5.27MB/s]\nmodel.safetensors: 100%|████████████████████| 90.9M/90.9M [00:00<00:00, 244MB/s]\ntokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 2.63MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 13.3MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 46.3MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 857kB/s]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n1_Pooling/config.json: 100%|███████████████████| 190/190 [00:00<00:00, 1.42MB/s]\nBatches: 100%|██████████████████████████████████| 89/89 [00:02<00:00, 40.59it/s]\nBatches: 100%|██████████████████████████████████| 23/23 [00:00<00:00, 53.62it/s]\n\u001b[38;20m2024-10-08 18:24:12,765 - logger_config - INFO - SentenceTransformer embeddings generated successfully. (embeddings.py:59)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from embeddings import TextEmbeddings\n","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:24:15.304950Z","iopub.execute_input":"2024-10-08T18:24:15.305269Z","iopub.status.idle":"2024-10-08T18:24:31.119550Z","shell.execute_reply.started":"2024-10-08T18:24:15.305236Z","shell.execute_reply":"2024-10-08T18:24:31.118645Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# tfidf embeddings\ntfidf_train, tfidf_test = embeddings.apply_tfidf()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:24:31.120652Z","iopub.execute_input":"2024-10-08T18:24:31.121221Z","iopub.status.idle":"2024-10-08T18:24:34.291713Z","shell.execute_reply.started":"2024-10-08T18:24:31.121184Z","shell.execute_reply":"2024-10-08T18:24:34.290768Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"\u001b[38;20m2024-10-08 18:24:33,827 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:34,287 - logger_config - INFO - TF-IDF embeddings generated successfully. (embeddings.py:23)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# word2vec embeddings\nword2vec_train, word2vec_test = embeddings.apply_word2vec()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:24:34.293907Z","iopub.execute_input":"2024-10-08T18:24:34.294642Z","iopub.status.idle":"2024-10-08T18:24:35.670848Z","shell.execute_reply.started":"2024-10-08T18:24:34.294561Z","shell.execute_reply":"2024-10-08T18:24:35.670094Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-08 18:24:34,315 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:35,663 - logger_config - INFO - Word2Vec embeddings generated successfully. (embeddings.py:44)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\n\nsentence_transformer_train, sentence_transformer_test = embeddings_contextual.apply_sentence_transformer()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:24:35.672092Z","iopub.execute_input":"2024-10-08T18:24:35.672467Z","iopub.status.idle":"2024-10-08T18:24:39.455191Z","shell.execute_reply.started":"2024-10-08T18:24:35.672422Z","shell.execute_reply":"2024-10-08T18:24:39.454291Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-08 18:24:35,694 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-08 18:24:35,719 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/89 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b51eacc9c6442f8372ae03a9f4ca6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689d36ac055146eb899e2aa580a5dafe"}},"metadata":{}},{"name":"stderr","text":"\u001b[38;20m2024-10-08 18:24:39,446 - logger_config - INFO - SentenceTransformer embeddings generated successfully. (embeddings.py:59)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB  # Import Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndef train_and_evaluate_naive_bayes(X_train, y_train, X_test, y_test, embedding_name):\n    \"\"\"\n    Train and evaluate Naive Bayes model on the given data.\n    Arguments:\n    - X_train: Training features\n    - y_train: Training labels\n    - X_test: Test features\n    - y_test: Test labels\n    - embedding_name: Name of the embedding used\n    \n    Returns:\n    - results: Dictionary containing model and its performance metrics\n    \"\"\"\n\n    # Split the training data\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n    # Initialize Naive Bayes model\n    model = GaussianNB()  # Change to Naive Bayes\n    \n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on validation and test sets\n    y_val_pred = model.predict(X_val)\n    y_test_pred = model.predict(X_test)\n\n    # Function to calculate evaluation metrics\n    def get_metrics(y_true, y_pred):\n        return {\n            'Precision': precision_score(y_true, y_pred, average='weighted'),\n            'Recall': recall_score(y_true, y_pred, average='weighted'),\n            'F1 Score': f1_score(y_true, y_pred, average='weighted'),\n            'Accuracy': accuracy_score(y_true, y_pred)\n        }\n\n    # Compute metrics for validation and test sets\n    val_metrics = get_metrics(y_val, y_val_pred)\n    test_metrics = get_metrics(y_test, y_test_pred)\n\n    # Print results for each set of embeddings\n    print(f\"\\nResults for {embedding_name} embeddings - Naive Bayes:\")\n    print(\"Validation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    print(\"\\nTest Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    return model, test_metrics","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:31:09.307683Z","iopub.execute_input":"2024-10-08T18:31:09.308660Z","iopub.status.idle":"2024-10-08T18:31:09.319067Z","shell.execute_reply.started":"2024-10-08T18:31:09.308586Z","shell.execute_reply":"2024-10-08T18:31:09.318078Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"all_results = {}\n\nprint(\"Running Naive Bayes on TF-IDF Embeddings...\")\nall_results['TF-IDF'] = train_and_evaluate_naive_bayes(\n    tfidf_train, train_data['label'], tfidf_test, test_data['label'], \"TF-IDF\"\n)\n\nprint(\"\\nRunning Naive Bayes on Word2Vec Embeddings...\")\nall_results['Word2Vec'] = train_and_evaluate_naive_bayes(\n    word2vec_train, train_data['label'], word2vec_test, test_data['label'], \"Word2Vec\"\n)\n    \nprint(\"\\nRunning Naive Bayes on SentenceTransformer Embeddings...\")\nall_results['SentenceTransformer'] = train_and_evaluate_naive_bayes(\n    sentence_transformer_train, train_data_contextual['label'],\n    sentence_transformer_test, test_data_contextual['label'], \"SentenceTransformer\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T18:31:09.711179Z","iopub.execute_input":"2024-10-08T18:31:09.711579Z","iopub.status.idle":"2024-10-08T18:31:10.960920Z","shell.execute_reply.started":"2024-10-08T18:31:09.711541Z","shell.execute_reply":"2024-10-08T18:31:10.959963Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Running Naive Bayes on TF-IDF Embeddings...\n\nResults for TF-IDF embeddings - Naive Bayes:\nValidation Metrics:\nPrecision: 0.6288\nRecall: 0.6303\nF1 Score: 0.6273\nAccuracy: 0.6303\n\nTest Metrics:\nPrecision: 0.5501\nRecall: 0.5510\nF1 Score: 0.5459\nAccuracy: 0.5510\n\nRunning Naive Bayes on Word2Vec Embeddings...\n\nResults for Word2Vec embeddings - Naive Bayes:\nValidation Metrics:\nPrecision: 0.5947\nRecall: 0.5968\nF1 Score: 0.5940\nAccuracy: 0.5968\n\nTest Metrics:\nPrecision: 0.5595\nRecall: 0.5566\nF1 Score: 0.5383\nAccuracy: 0.5566\n\nRunning Naive Bayes on SentenceTransformer Embeddings...\n\nResults for SentenceTransformer embeddings - Naive Bayes:\nValidation Metrics:\nPrecision: 0.7041\nRecall: 0.7042\nF1 Score: 0.7041\nAccuracy: 0.7042\n\nTest Metrics:\nPrecision: 0.7260\nRecall: 0.7259\nF1 Score: 0.7259\nAccuracy: 0.7259\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}