{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-09T06:25:11.665071Z","iopub.execute_input":"2024-10-09T06:25:11.665861Z","iopub.status.idle":"2024-10-09T06:25:12.657298Z","shell.execute_reply.started":"2024-10-09T06:25:11.665821Z","shell.execute_reply":"2024-10-09T06:25:12.656540Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/deepakachu5114/ML-Project.git\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:12.659319Z","iopub.execute_input":"2024-10-09T06:25:12.660174Z","iopub.status.idle":"2024-10-09T06:25:14.422854Z","shell.execute_reply.started":"2024-10-09T06:25:12.660124Z","shell.execute_reply":"2024-10-09T06:25:14.421733Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'ML-Project'...\nremote: Enumerating objects: 35, done.\u001b[K\nremote: Counting objects: 100% (35/35), done.\u001b[K\nremote: Compressing objects: 100% (27/27), done.\u001b[K\nremote: Total 35 (delta 7), reused 21 (delta 3), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (35/35), 1.33 MiB | 16.97 MiB/s, done.\nResolving deltas: 100% (7/7), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd \"/kaggle/working/ML-Project\"","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:14.424697Z","iopub.execute_input":"2024-10-09T06:25:14.425132Z","iopub.status.idle":"2024-10-09T06:25:14.432594Z","shell.execute_reply.started":"2024-10-09T06:25:14.425082Z","shell.execute_reply":"2024-10-09T06:25:14.431580Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:14.433907Z","iopub.execute_input":"2024-10-09T06:25:14.434255Z","iopub.status.idle":"2024-10-09T06:25:27.263394Z","shell.execute_reply.started":"2024-10-09T06:25:14.434213Z","shell.execute_reply":"2024-10-09T06:25:27.262263Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.3->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.1.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile log_reg.py\nfrom preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\n\nfrom sentence_transformers import SentenceTransformer\n\n\npreproc_noncontextual = PreprocessForNonContextualEmbeddings()\npreproc_noncontextual.save()\n\npreproc_contextual = PreprocessForContextualEmbeddings()\npreproc_contextual.save()\n\n# generating embeddings\n\nfrom embeddings import TextEmbeddings\n\n# preprocessed data\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# tfidf embeddings\ntfidf_train, tfidf_test = embeddings.apply_tfidf()\n\n# word2vec embeddings\nword2vec_train, word2vec_test = embeddings.apply_word2vec()\n\n# stransformers embeddings\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\n\nsentence_transformer_train, sentence_transformer_test = embeddings_contextual.apply_sentence_transformer()\n\nmodel_mpnet = SentenceTransformer('all-mpnet-base-v2')\ntrain_embeddings_mpnet = model_mpnet.encode(train_data_contextual['text'].tolist(), show_progress_bar=True)\ntest_embeddings_mpnet = model_mpnet.encode(test_data_contextual['text'].tolist(), show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:27.266282Z","iopub.execute_input":"2024-10-09T06:25:27.266627Z","iopub.status.idle":"2024-10-09T06:25:27.274448Z","shell.execute_reply.started":"2024-10-09T06:25:27.266590Z","shell.execute_reply":"2024-10-09T06:25:27.273526Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing log_reg.py\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd \"/kaggle/working/ML-Project/data\"","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:27.275699Z","iopub.execute_input":"2024-10-09T06:25:27.276047Z","iopub.status.idle":"2024-10-09T06:25:27.288124Z","shell.execute_reply.started":"2024-10-09T06:25:27.276006Z","shell.execute_reply":"2024-10-09T06:25:27.287246Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project/data\n","output_type":"stream"}]},{"cell_type":"code","source":"mkdir preprocessed","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:27.289361Z","iopub.execute_input":"2024-10-09T06:25:27.289815Z","iopub.status.idle":"2024-10-09T06:25:28.284111Z","shell.execute_reply.started":"2024-10-09T06:25:27.289772Z","shell.execute_reply":"2024-10-09T06:25:28.282685Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ncd \"/kaggle/working/ML-Project\"","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:28.288701Z","iopub.execute_input":"2024-10-09T06:25:28.289130Z","iopub.status.idle":"2024-10-09T06:25:28.296862Z","shell.execute_reply.started":"2024-10-09T06:25:28.289081Z","shell.execute_reply":"2024-10-09T06:25:28.295736Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/working/ML-Project\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:28.298332Z","iopub.execute_input":"2024-10-09T06:25:28.298714Z","iopub.status.idle":"2024-10-09T06:25:39.865177Z","shell.execute_reply.started":"2024-10-09T06:25:28.298681Z","shell.execute_reply":"2024-10-09T06:25:39.864091Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.1.1)\nRequirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.19.3->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!python log_reg.py","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:25:39.866630Z","iopub.execute_input":"2024-10-09T06:25:39.866964Z","iopub.status.idle":"2024-10-09T06:27:39.557881Z","shell.execute_reply.started":"2024-10-09T06:25:39.866927Z","shell.execute_reply":"2024-10-09T06:27:39.556758Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\u001b[38;20m2024-10-09 06:26:04,624 - logger_config - INFO - Preprocessing for non-contextual embeddings initialized. (preprocess.py:20)\u001b[0m\n\u001b[38;20m2024-10-09 06:26:44,892 - logger_config - INFO - Preprocessing for non-contextual embeddings completed. (preprocess.py:85)\u001b[0m\n\u001b[38;20m2024-10-09 06:26:44,981 - logger_config - INFO - Preprocessing for contextual embeddings initialized. (preprocess.py:92)\u001b[0m\n\u001b[38;20m2024-10-09 06:26:45,105 - logger_config - INFO - Preprocessing for contextual embeddings completed. (preprocess.py:138)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:08,857 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:09,378 - logger_config - INFO - TF-IDF embeddings generated successfully. (embeddings.py:23)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:10,921 - logger_config - INFO - Word2Vec embeddings generated successfully. (embeddings.py:44)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:10,950 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 2.82MB/s]\nconfig_sentence_transformers.json: 100%|███████| 116/116 [00:00<00:00, 1.01MB/s]\nREADME.md: 100%|███████████████████████████| 10.7k/10.7k [00:00<00:00, 41.4MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 319kB/s]\nconfig.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 4.07MB/s]\nmodel.safetensors: 100%|████████████████████| 90.9M/90.9M [00:00<00:00, 244MB/s]\ntokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 2.13MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 13.4MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 23.8MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 919kB/s]\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n1_Pooling/config.json: 100%|███████████████████| 190/190 [00:00<00:00, 1.54MB/s]\nBatches: 100%|██████████████████████████████████| 89/89 [00:02<00:00, 40.81it/s]\nBatches: 100%|██████████████████████████████████| 23/23 [00:00<00:00, 53.16it/s]\n\u001b[38;20m2024-10-09 06:27:17,728 - logger_config - INFO - SentenceTransformer embeddings generated successfully. (embeddings.py:59)\u001b[0m\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 2.77MB/s]\nconfig_sentence_transformers.json: 100%|████████| 116/116 [00:00<00:00, 918kB/s]\nREADME.md: 100%|███████████████████████████| 10.6k/10.6k [00:00<00:00, 39.2MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 370kB/s]\nconfig.json: 100%|█████████████████████████████| 571/571 [00:00<00:00, 4.25MB/s]\nmodel.safetensors: 100%|██████████████████████| 438M/438M [00:01<00:00, 296MB/s]\ntokenizer_config.json: 100%|███████████████████| 363/363 [00:00<00:00, 2.58MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 29.1MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 3.72MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 239/239 [00:00<00:00, 2.12MB/s]\n1_Pooling/config.json: 100%|███████████████████| 190/190 [00:00<00:00, 1.16MB/s]\nBatches: 100%|██████████████████████████████████| 89/89 [00:11<00:00,  7.48it/s]\nBatches: 100%|██████████████████████████████████| 23/23 [00:02<00:00,  7.84it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from embeddings import TextEmbeddings","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:27:39.559440Z","iopub.execute_input":"2024-10-09T06:27:39.559815Z","iopub.status.idle":"2024-10-09T06:27:55.245364Z","shell.execute_reply.started":"2024-10-09T06:27:39.559765Z","shell.execute_reply":"2024-10-09T06:27:55.244500Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# tfidf embeddings\ntfidf_train, tfidf_test = embeddings.apply_tfidf()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:27:55.246485Z","iopub.execute_input":"2024-10-09T06:27:55.247108Z","iopub.status.idle":"2024-10-09T06:27:58.514107Z","shell.execute_reply.started":"2024-10-09T06:27:55.247071Z","shell.execute_reply":"2024-10-09T06:27:58.513257Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:27:57,999 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:58,510 - logger_config - INFO - TF-IDF embeddings generated successfully. (embeddings.py:23)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\n\n# word2vec embeddings\nword2vec_train, word2vec_test = embeddings.apply_word2vec()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:27:58.515245Z","iopub.execute_input":"2024-10-09T06:27:58.515557Z","iopub.status.idle":"2024-10-09T06:27:59.904662Z","shell.execute_reply.started":"2024-10-09T06:27:58.515522Z","shell.execute_reply":"2024-10-09T06:27:59.903906Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:27:58,537 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:59,896 - logger_config - INFO - Word2Vec embeddings generated successfully. (embeddings.py:44)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\n\nsentence_transformer_train, sentence_transformer_test = embeddings_contextual.apply_sentence_transformer()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:27:59.908366Z","iopub.execute_input":"2024-10-09T06:27:59.909051Z","iopub.status.idle":"2024-10-09T06:28:03.661512Z","shell.execute_reply.started":"2024-10-09T06:27:59.909015Z","shell.execute_reply":"2024-10-09T06:28:03.660754Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:27:59,930 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:27:59,957 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/89 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ebe2f5e80b4982a2df0ba0504354f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6024bae9cc104b469dbc151efbc46a16"}},"metadata":{}},{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:28:03,653 - logger_config - INFO - SentenceTransformer embeddings generated successfully. (embeddings.py:59)\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from preprocess import PreprocessForNonContextualEmbeddings, PreprocessForContextualEmbeddings\nimport pandas as pd\nfrom constants import SAVEPATH\nfrom sentence_transformers import SentenceTransformer\ntrain_data = pd.read_csv(f\"{SAVEPATH}/train_noncontextual_preprocessed.csv\")\ntest_data = pd.read_csv(f\"{SAVEPATH}/test_noncontextual_preprocessed.csv\")\n\nembeddings = TextEmbeddings(train_data, test_data)\ntrain_data_contextual = pd.read_csv(f\"{SAVEPATH}/train_contextual_preprocessed.csv\")\ntest_data_contextual = pd.read_csv(f\"{SAVEPATH}/test_contextual_preprocessed.csv\")\nembeddings_contextual = TextEmbeddings(train_data_contextual, test_data_contextual)\nmodel_mpnet = SentenceTransformer('all-mpnet-base-v2')\ntrain_embeddings_mpnet = model_mpnet.encode(train_data_contextual['text'].tolist(), show_progress_bar=True)\ntest_embeddings_mpnet = model_mpnet.encode(test_data_contextual['text'].tolist(), show_progress_bar=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:28:20.645533Z","iopub.execute_input":"2024-10-09T06:28:20.646706Z","iopub.status.idle":"2024-10-09T06:28:36.994439Z","shell.execute_reply.started":"2024-10-09T06:28:20.646650Z","shell.execute_reply":"2024-10-09T06:28:36.993487Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[38;20m2024-10-09 06:28:20,670 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n\u001b[38;20m2024-10-09 06:28:20,694 - logger_config - INFO - TextEmbeddings initialized. (embeddings.py:12)\u001b[0m\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/89 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adc45dd6e8494bc091ee5bf57bfbaf1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"029facad9a9b4ea3aa38de6aead4026b"}},"metadata":{}}]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n# from sklearn.linear_model import LogisticRegression\n# def train_and_evaluate_logistic_regression(X_train, y_train, X_test, y_test, embedding_name):\n#     \"\"\"\n#     Train and evaluate Logistic Regression model on the given data.\n#     Arguments:\n#     - X_train: Training features\n#     - y_train: Training labels\n#     - X_test: Test features\n#     - y_test: Test labels\n#     - embedding_name: Name of the embedding used\n    \n#     Returns:\n#     - results: Dictionary containing model and its performance metrics\n#     \"\"\"\n\n#     # Split the training data\n#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n#     # Initialize Logistic Regression model\n#     model = LogisticRegression(max_iter=1000, random_state=42)\n    \n#     # Train the model\n#     model.fit(X_train, y_train)\n\n#     # Make predictions on validation and test sets\n#     y_val_pred = model.predict(X_val)\n#     y_test_pred = model.predict(X_test)\n\n#     # Function to calculate evaluation metrics\n#     def get_metrics(y_true, y_pred):\n#         return {\n#             'Precision': precision_score(y_true, y_pred),\n#             'Recall': recall_score(y_true, y_pred),\n#             'F1 Score': f1_score(y_true, y_pred),\n#             'Accuracy': accuracy_score(y_true, y_pred)\n#         }\n\n#     # Compute metrics for validation and test sets\n#     val_metrics = get_metrics(y_val, y_val_pred)\n#     test_metrics = get_metrics(y_test, y_test_pred)\n\n#     # Print results for each set of embeddings\n#     print(f\"\\nResults for {embedding_name} embeddings - Logistic Regression:\")\n#     print(\"Validation Metrics:\")\n#     for metric, value in val_metrics.items():\n#         print(f\"{metric}: {value:.4f}\")\n\n#     print(\"\\nTest Metrics:\")\n#     for metric, value in test_metrics.items():\n#         print(f\"{metric}: {value:.4f}\")\n\n#     return model, test_metrics\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\n\ndef train_and_evaluate_logistic_regression(X_train, y_train, X_test, y_test, embedding_name):\n    \"\"\"\n    Train and evaluate Logistic Regression model on the given data.\n    Arguments:\n    - X_train: Training features\n    - y_train: Training labels\n    - X_test: Test features\n    - y_test: Test labels\n    - embedding_name: Name of the embedding used\n    \n    Returns:\n    - results: Dictionary containing model and its performance metrics\n    \"\"\"\n\n    # Split the training data\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n    # Initialize Logistic Regression model\n    model = LogisticRegression(max_iter=1000, random_state=42)\n    \n    # Train the model\n    model.fit(X_train, y_train)\n\n    # Make predictions on training, validation, and test sets\n    y_train_pred = model.predict(X_train)\n    y_val_pred = model.predict(X_val)\n    y_test_pred = model.predict(X_test)\n\n    # Function to calculate evaluation metrics\n    def get_metrics(y_true, y_pred):\n        return {\n            'Precision': precision_score(y_true, y_pred),\n            'Recall': recall_score(y_true, y_pred),\n            'F1 Score': f1_score(y_true, y_pred),\n            'Accuracy': accuracy_score(y_true, y_pred)\n        }\n\n    # Compute metrics for training, validation, and test sets\n    train_metrics = get_metrics(y_train, y_train_pred)\n    val_metrics = get_metrics(y_val, y_val_pred)\n    test_metrics = get_metrics(y_test, y_test_pred)\n\n    # Print results for each set of embeddings\n    print(f\"\\nResults for {embedding_name} embeddings - Logistic Regression:\")\n\n    print(\"\\nTraining Metrics:\")\n    for metric, value in train_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    print(\"\\nValidation Metrics:\")\n    for metric, value in val_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    print(\"\\nTest Metrics:\")\n    for metric, value in test_metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n\n    return model, test_metrics\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:28:41.610813Z","iopub.execute_input":"2024-10-09T06:28:41.611260Z","iopub.status.idle":"2024-10-09T06:28:41.624091Z","shell.execute_reply.started":"2024-10-09T06:28:41.611170Z","shell.execute_reply":"2024-10-09T06:28:41.623000Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"all_results = {}\n\nprint(\"Running Logistic Regression on TF-IDF Embeddings...\")\nall_results['TF-IDF'] = train_and_evaluate_logistic_regression(\n    tfidf_train, train_data['label'], tfidf_test, test_data['label'], \"TF-IDF\"\n)\n\nprint(\"\\nRunning Logistic Regression on Word2Vec Embeddings...\")\nall_results['Word2Vec'] = train_and_evaluate_logistic_regression(\n    word2vec_train, train_data['label'], word2vec_test, test_data['label'], \"Word2Vec\"\n)\n\n# print(\"\\nRunning Logistic Regression on SentenceTransformer Embeddings...\")\n# for model_name, embeddings in sentence_transformer_embeddings.items():\n#     if embeddings is not None:\n#         print(f\"\\nRunning Logistic Regression on SentenceTransformer-{model_name} Embeddings...\")\n#         all_results[f'SentenceTransformer-{model_name}'] = {\n#             'Logistic Regression': train_and_evaluate_logistic_regression(\n#                 embeddings['train_embeddings'], train_data_contextual['label'],\n#                 embeddings['test_embeddings'], test_data_contextual['label'],\n#                 f\"SentenceTransformer-{model_name}\"\n#             )\n#         }\n\nprint(\"\\nRunning Logistic Regression on SentenceTransformer Embeddings...\")\nall_results['SentenceTransformer'] = train_and_evaluate_logistic_regression(\n    sentence_transformer_train, train_data_contextual['label'],\n    sentence_transformer_test, test_data_contextual['label'], \"SentenceTransformer\"\n)\n\nprint(\"\\nRunning Logistic Regression on all-mpnet-base-v2 Embeddings...\")\nall_results['all-mpnet-base-v2'] = train_and_evaluate_logistic_regression(\n    train_embeddings_mpnet, train_data_contextual['label'],\n    test_embeddings_mpnet, test_data_contextual['label'], \"all-mpnet-base-v2\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T06:28:44.185099Z","iopub.execute_input":"2024-10-09T06:28:44.185928Z","iopub.status.idle":"2024-10-09T06:28:46.518244Z","shell.execute_reply.started":"2024-10-09T06:28:44.185873Z","shell.execute_reply":"2024-10-09T06:28:46.517059Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Running Logistic Regression on TF-IDF Embeddings...\n\nResults for TF-IDF embeddings - Logistic Regression:\n\nTraining Metrics:\nPrecision: 0.8839\nRecall: 0.9205\nF1 Score: 0.9019\nAccuracy: 0.8956\n\nValidation Metrics:\nPrecision: 0.7459\nRecall: 0.7410\nF1 Score: 0.7434\nAccuracy: 0.7254\n\nTest Metrics:\nPrecision: 0.7133\nRecall: 0.8022\nF1 Score: 0.7551\nAccuracy: 0.7315\n\nRunning Logistic Regression on Word2Vec Embeddings...\n\nResults for Word2Vec embeddings - Logistic Regression:\n\nTraining Metrics:\nPrecision: 0.6402\nRecall: 0.6678\nF1 Score: 0.6537\nAccuracy: 0.6313\n\nValidation Metrics:\nPrecision: 0.6445\nRecall: 0.6361\nF1 Score: 0.6403\nAccuracy: 0.6162\n\nTest Metrics:\nPrecision: 0.5662\nRecall: 0.6721\nF1 Score: 0.6146\nAccuracy: 0.5650\n\nRunning Logistic Regression on SentenceTransformer Embeddings...\n\nResults for SentenceTransformer embeddings - Logistic Regression:\n\nTraining Metrics:\nPrecision: 0.8106\nRecall: 0.8428\nF1 Score: 0.8264\nAccuracy: 0.8154\n\nValidation Metrics:\nPrecision: 0.7695\nRecall: 0.8098\nF1 Score: 0.7891\nAccuracy: 0.7676\n\nTest Metrics:\nPrecision: 0.7604\nRecall: 0.7913\nF1 Score: 0.7756\nAccuracy: 0.7636\n\nRunning Logistic Regression on all-mpnet-base-v2 Embeddings...\n\nResults for all-mpnet-base-v2 embeddings - Logistic Regression:\n\nTraining Metrics:\nPrecision: 0.8285\nRecall: 0.8495\nF1 Score: 0.8389\nAccuracy: 0.8300\n\nValidation Metrics:\nPrecision: 0.7630\nRecall: 0.7705\nF1 Score: 0.7667\nAccuracy: 0.7482\n\nTest Metrics:\nPrecision: 0.7796\nRecall: 0.7859\nF1 Score: 0.7827\nAccuracy: 0.7748\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}